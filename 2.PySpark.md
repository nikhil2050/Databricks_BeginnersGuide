Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27517062

# Section 11, 12, 13: Data Ingestion - CSV, JSON, Multiple Files

|      |    |                     |    |                |    |                     |    |         | 
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  |    |                     |    |                |    |                     |    | Parquet |
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | AVRO    |
| XML  |    | DataFrameReader API |    | DataFrame API  |    | DataFrameWriter API |    | Delta   |
| JDBC |    |                     |    | Data Types     |    |                     |    | JDBC    |
| ...  |    |                     |    | Row            |    |                     |    | ...     |
|      |    |                     |    | Column         |    |                     |    |         |
|      |    |                     |    | Functions      |    |                     |    |         |
|      |    |                     |    | Window         |    |                     |    |         |
|      |    |                     |    | Grouping       |    |                     |    |         |
|      |    |                     |    |                |    |                     |    |         |

| File Name    | File type                   |
|--------------|-----------------------------|
| Circuits     | CSV                         |
| Races        | CSV                         |
| Constructors | Single line JSON            |
| Results      | Single line JSON            |
| Drivers      | Single line Nested JSON     |
| Pitstops     | Multi line JSON             |
| Laptimes     | Split CSV Files             |
| Qualifying   | Split Multi line JSON Files |

# Section 11: Data Ingestion - CSV
## 11.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

## 11.2 DataFrameReader, Specify Schema, Select Columns, Rename Columns, Add Column, DataFrameWriter

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html

```python
# ingestion/1.ingest_circuits_file
# ingestion/2.ingest_races_file

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import col, current_timestamp

# --------------------------------------------------------------------------
# Step 1a - READ A CSV FILE USING SPARK DATAFRAMEREADER (Find Schema Automatically)
circuits_df = spark.read \
                  .option("header", True) \                       # Specify that there is a header present
                  .option("inferSchema", True) \                  # Set appropriate schema (Different job runs in background)
                  .schema(circuits_schema) \                      # Specify schema
                  .csv("dbfs://mnt/formula1dl/raw/circuits.csv")
# OR
# Step 1b - READ A CSV FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
circuits_schema = StructType(fields=[StructField("circuitId",   IntegerType(), False),
                                     StructField("circuitRef",  StringType(),  True),
                                     StructField("name",        StringType(),  True),
                                     StructField("location",    StringType(),  True),
                                     StructField("country",     StringType(),  True),
                                     StructField("lat",         DoubleType(),  True),
                                     StructField("lng",         DoubleType(),  True),
                                     StructField("alt",         IntegerType(), True),
                                     StructField("url",         StringType(),  True)
                                     #StructField("date",       DateType(),    True),
                                     #StructField("time",       StringType(),  True),
])
circuits_df = spark.read \
                  .option("header", True) \
                  .schema(circuits_schema) \
                  .csv("/mnt/formula1dl/raw/circuits.csv")

type(circuits_df)    # DataFrame

circuits_df.show()   # Displays all data in DF in Text format
  
display(circuits_df) # Displays all data in DF in Table format

circuits_df.printSchema()

# --------------------------------------------------------------------------
# Step 2 - SELECT ONLY REQUIRED COLUMNS
circuits_selected_df = circuits_df.select(\
                                    col("circuitId"), col("circuitRef"), col("name"), col("location"),
                                    col("country"), col("lat"), col("lng"), col("alt")
)
# OR
circuits_selected_df = circuits_df.select(\
                                    "circuitId", "circuitRef", "name" , "location",
                                    "country", "lat", "lng", "alt"
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df.circuitId, circuits_df.circuitRef, circuits_df.name, circuits_df.location,
                                    circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df["circuitId"], circuits_df["circuitRef"], circuits_df["name"], circuits_df["location"],
                                    circuits_df["country"], circuits_df["lat"], circuits_df["lng"], circuits_df["alt"]
)

# --------------------------------------------------------------------------
# Step 3 - RENAME EXISTING COLUMNS AS REQUIRED
circuits_renamed_df = circuits_selected_df
                                  .withColumnRenamed("circuitId", "circuit_id") \
                                  .withColumnRenamed("circuitRef", "circuit_ref") \
                                  .withColumnRenamed("lat", "latitude") \
                                  .withColumnRenamed("lng", "longitude") \
                                  .withColumnRenamed("alt", "altitude")

# --------------------------------------------------------------------------
# Step 4 - ADD NEW COLUMN (ingestion_date, gender) TO THE DATAFRAME
circuits_final_df = circuits_renamed_df
                                  .withColumn("ingestion_date", current_timestamp()) \
                                  #.withColumn("ingestion_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))
                                  .withColumns("gender", lit("Male"))

# --------------------------------------------------------------------------
# Step 5 - WRITE DATA TO DATALAKE AS PARQUET
# NOTE: If you do not have Databricks mounts, use the cluster-scoped authentication and abfss protocol as described in Section 6. Folder path will be:
# abfss://processed@formula1dl.dfs.core.windows.net/circuits
circuits_final_df.write \
                .mode("overwrite") \
                #.partitionBy("country")
                .parquet("/mnt/formula1dl/processed/circuits")

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))    # Read the output Parquet file

```

# Section 12: Data Ingestion - JSON
## 12.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html
  
## 12.2 Single Line JSON
JSON File:\
```json
{"constructorId":1, "constructorRef":"mclaren", "name":"McLaren", "nationality":"British", "url":"www.mclaren.com", "fullname":{"fname":"Mc", "lname":"Laren"}}
{"constructorId":2, "constructorRef":"landrover", "name":"LandRover", "nationality":"Indian", "url":"www.landrover.com", "fullname":{"fname":"Land", "lname":"Rover"}}
```

```python
# ingestion/3.ingest_constructors_file
# ingestion/4.ingest_drivers_file

from pyspark.sql.functions import col, current_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# Step 1a - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# Step 1b - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
#inner_fullname_schema = StructType(fields=[StructField("fname", StringType(), True),
#                                      StructField("lname", StringType(), True)
# ])
constructors_schema = StructType(fields=[StructField("constructorId",  IntegerType(), True), \
                                         StructField("constructorRef", StringType(), True), \
                                         StructField("name",           StringType(), True), \
                                         StructField("nationality",    StringType(), True), \
                                         StructField("url",            StringType(), True), \
                                         # StructField("time",         StringType(), True), \        # Syntax: "17:05:23"
                                         # StructField("fullname", inner_fullname_schema), \         # Nested JSON
])
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# --------------------------------------------------------------------------
# Step 2 - DROP UNWANTED COLUMNS FROM DATAFRAME
constructor_dropped_df = constructor_df.drop(col('url'))

# --------------------------------------------------------------------------
# Step 3 - RENAME COLUMNS AND ADD COLUMNS(ingestion_date)
constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())
                                             #.withColumn("name", concat(col("fullname.fname"), lit(" "), col("fullname.lname")))

# --------------------------------------------------------------------------
# Step 4 - WRITE DATA TO DATALAKE AS PARQUET
constructor_final_df.write  \
                      .mode("overwrite") \
                      .parquet("/mnt/formula1dl/processed/constructors")
```

## 12.2 Multi-Line JSON
```json
[
  {
    "raceId": 841,
    "driverId": 153,
    "stop": 1,
    "lap": 1,
    "time": "17:05:23",
    "duration": 26.898,
    "milliseconds": 26898
  },
  {
    "raceId": 841,
    "driverId": 30,
    "stop": 1,
    "lap": 1,
    "time": "17:05:52",
    "duration": 25.091,
    "milliseconds": 25091
  }
]
```
```python
# ingestion/6.ingest_pit_stops_file

pit_stops_df = spark.read \
                    .schema(pit_stops_schema) \
                    .option("multiLine", True) \                # Handle Multiline JSON
                    .json("/mnt/formula1dl/raw/pit_stops.json")
```

# Section 13: Data Ingestion - Multiple Files
## 13.1 Multiple Split CSV Files
- CSV file is split into multiple files, with no headers.
```
lap_times
+ lap_times_split_1.csv
+ lap_times_split_2.csv
+ lap_times_split_3.csv
```

lap_times_split_1.csv
```csv
841,20,1,1,"1:38:109",98109
841,20,2,1,"1:33:006",93006
841,20,3,1,"1:32:713",92713
```

Schema:
```
raceId        int(11)
driverId      int(11)
lap           int(11)
position      int(11)
time          varchar(255)
milliseconds  int(11)
```

```python
# ingestion/7.ingest_lap_times_file

lap_times_df = spark.read \
                    .schema(lap_times_schema) \
                    .csv("/mnt/formula1dl/raw/lap_times")                      # Handle Multiple CSV Files
# OR
lap_times_df = spark.read \
                    .schema(lap_times_schema) \
                    .csv("/mnt/formula1dl/raw/lap_times/lap_times_split*.csv")  # Handle Multiple CSV Files
```

## 13.2 Multiple Split Multiline JSON Files
- JSON file is split into multiple files, with no headers.
```
qualifying
+ qualifying_split_1.csv
+ qualifying_split_2.csv
+ qualifying_split_3.csv
```

qualifying_split_1.csv
```json
[
  {
    "qualifyId": 1,
    "raceId": 18,
    "driverId": 1,
    "constructorId": 1,
    "number": 22,
    "position": 1,
    "q1": "1:26:572",
    "q2": "1:25:187",
    "q3": "1:26:714"
  },
  {
    "qualifyId": 2,
    "raceId": 18,
    "driverId": 9,
    "constructorId": 2,
    "number": 4,
    "position": 2,
    "q1": "1:26:103",
    "q2": "1:25:315",
    "q3": "1:26:869"
  }
]
```

```python
# ingestion/8.ingest_qualifying_file

qualifying_df = spark.read \
                        .schema(qualifying_schema) \
                        .option("multiLine", True) \              # Handle Multiline JSON
                        .json("/mnt/formula1dl/raw/qualifying")   # Handle Multiple JSON Files
```

# Section 15: Transformation - Filter & Join

## 15.1 Filter
**Reference:**
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.filter.html#pyspark.sql.DataFrame.filter

```python
# /demo/1.filter_demo

%run "/includes/configuration"
races_df = spark.read.parquet(f"{processed_folder_path}/races")

races_filtered_df = races_df.filter("race_year = 2019 and round <=5").collect()
# OR
races_filtered_df = races_df.filter( (races_df.race_year = 2019) & (races_df.round <= 5)).collect()
# OR
races_filtered_df = races_df.filter( (races_df["race_year"] == 2019) & (races_df["round"] <= 5) ).collect()
```

## 15.2 Joins
**Reference:**
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join

```python
# /demo/2.join_demo
%run "/includes/configuration"

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
                        .filter("circuit_id < 70") \
                        .withColumnRenamed("name", "circuit_name")
races_df = spark.read.parquet(f"{processed_folder_path}/races")
                        .filter("race_year = 2019") \
                        .withColumnRenamed("name", "race_name")

# Inner Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Left Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Right Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Full Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Semi Joins
race_circuits_df = circuits_df
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 
# Anti Joins
race_circuits_df = races_df
                        .join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 
# Cross Joins
race_circuits_df = races_df
                        .crossJoin(circuits_df)
```










# Section 16: Aggregations
Coming soon..

