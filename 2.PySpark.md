Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27517062

# Section 11, 12, 13: Data Ingestion - CSV, JSON, Multiple Files

|      |    |                     |    |                |    |                     |    |         | 
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  |    |                     |    |                |    |                     |    | Parquet |
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | AVRO    |
| XML  |    | DataFrameReader API |    | DataFrame API  |    | DataFrameWriter API |    | Delta   |
| JDBC |    |                     |    | Data Types     |    |                     |    | JDBC    |
| ...  |    |                     |    | Row            |    |                     |    | ...     |
|      |    |                     |    | Column         |    |                     |    |         |
|      |    |                     |    | Functions      |    |                     |    |         |
|      |    |                     |    | Window         |    |                     |    |         |
|      |    |                     |    | Grouping       |    |                     |    |         |
|      |    |                     |    |                |    |                     |    |         |

| File Name    | File type                   |
|--------------|-----------------------------|
| Circuits     | CSV                         |
| Races        | CSV                         |
| Constructors | Single line JSON            |
| Results      | Single line JSON            |
| Drivers      | Single line Nested JSON     |
| Pitstops     | Multi line JSON             |
| Laptimes     | Split CSV Files             |
| Qualifying   | Split Multi line JSON Files |

# Section 11: Data Ingestion - CSV
## 11.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

## 11.2 DataFrameReader, Specify Schema, Select Columns, Rename Columns, Add Column, DataFrameWriter

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html

```python
# ingestion/1.ingest_circuits_file
# ingestion/2.ingest_races_file

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import col, current_timestamp

# --------------------------------------------------------------------------
# Step 1a - READ A CSV FILE USING SPARK DATAFRAMEREADER (Find Schema Automatically)
circuits_df = spark.read \
                  .option("header", True) \                       # Specify that there is a header present
                  .option("inferSchema", True) \                  # Set appropriate schema (Different job runs in background)
                  .schema(circuits_schema) \                      # Specify schema
                  .csv("dbfs://mnt/formula1dl/raw/circuits.csv")
# OR
# Step 1b - READ A CSV FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
circuits_schema = StructType(fields=[StructField("circuitId",   IntegerType(), False),
                                     StructField("circuitRef",  StringType(),  True),
                                     StructField("name",        StringType(),  True),
                                     StructField("location",    StringType(),  True),
                                     StructField("country",     StringType(),  True),
                                     StructField("lat",         DoubleType(),  True),
                                     StructField("lng",         DoubleType(),  True),
                                     StructField("alt",         IntegerType(), True),
                                     StructField("url",         StringType(),  True)
                                     #StructField("date",       DateType(),    True),
                                     #StructField("time",       StringType(),  True),
])
circuits_df = spark.read \
                  .option("header", True) \
                  .schema(circuits_schema) \
                  .csv("/mnt/formula1dl/raw/circuits.csv")

type(circuits_df)    # DataFrame

circuits_df.show()   # Displays all data in DF in Text format
  
display(circuits_df) # Displays all data in DF in Table format

circuits_df.printSchema()

# --------------------------------------------------------------------------
# Step 2 - SELECT ONLY REQUIRED COLUMNS
circuits_selected_df = circuits_df.select(\
                                    col("circuitId"), col("circuitRef"), col("name"), col("location"),
                                    col("country"), col("lat"), col("lng"), col("alt")
)
# OR
circuits_selected_df = circuits_df.select(\
                                    "circuitId", "circuitRef", "name" , "location",
                                    "country", "lat", "lng", "alt"
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df.circuitId, circuits_df.circuitRef, circuits_df.name, circuits_df.location,
                                    circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df["circuitId"], circuits_df["circuitRef"], circuits_df["name"], circuits_df["location"],
                                    circuits_df["country"], circuits_df["lat"], circuits_df["lng"], circuits_df["alt"]
)

# --------------------------------------------------------------------------
# Step 3 - RENAME EXISTING COLUMNS AS REQUIRED
circuits_renamed_df = circuits_selected_df
                                  .withColumnRenamed("circuitId", "circuit_id") \
                                  .withColumnRenamed("circuitRef", "circuit_ref") \
                                  .withColumnRenamed("lat", "latitude") \
                                  .withColumnRenamed("lng", "longitude") \
                                  .withColumnRenamed("alt", "altitude")

# --------------------------------------------------------------------------
# Step 4 - ADD NEW COLUMN (ingestion_date, gender) TO THE DATAFRAME
circuits_final_df = circuits_renamed_df
                                  .withColumn("ingestion_date", current_timestamp()) \
                                  #.withColumn("ingestion_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))
                                  .withColumns("gender", lit("Male"))

# --------------------------------------------------------------------------
# Step 5 - WRITE DATA TO DATALAKE AS PARQUET
# NOTE: If you do not have Databricks mounts, use the cluster-scoped authentication and abfss protocol as described in Section 6. Folder path will be:
# abfss://processed@formula1dl.dfs.core.windows.net/circuits
circuits_final_df.write \
                .mode("overwrite") \
                #.partitionBy("country")
                .parquet("/mnt/formula1dl/processed/circuits")

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))    # Read the output Parquet file

```

# Section 12: Data Ingestion - JSON
## 12.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html
  
## 12.2 Single Line JSON
JSON File:\
```json
{"constructorId":1, "constructorRef":"mclaren", "name":"McLaren", "nationality":"British", "url":"www.mclaren.com", "fullname":{"fname":"Mc", "lname":"Laren"}}
{"constructorId":2, "constructorRef":"landrover", "name":"LandRover", "nationality":"Indian", "url":"www.landrover.com", "fullname":{"fname":"Land", "lname":"Rover"}}
```

```python
# ingestion/3.ingest_constructors_file
# ingestion/4.ingest_drivers_file

from pyspark.sql.functions import col, current_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# Step 1a - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# Step 1b - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
#inner_fullname_schema = StructType(fields=[StructField("fname", StringType(), True),
#                                      StructField("lname", StringType(), True)
# ])
constructors_schema = StructType(fields=[StructField("constructorId",  IntegerType(), True), \
                                         StructField("constructorRef", StringType(), True), \
                                         StructField("name",           StringType(), True), \
                                         StructField("nationality",    StringType(), True), \
                                         StructField("url",            StringType(), True), \
                                         # StructField("time",         StringType(), True), \        # Syntax: "17:05:23"
                                         # StructField("fullname", inner_fullname_schema), \         # Nested JSON
])
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# --------------------------------------------------------------------------
# Step 2 - DROP UNWANTED COLUMNS FROM DATAFRAME
constructor_dropped_df = constructor_df.drop(col('url'))

# --------------------------------------------------------------------------
# Step 3 - RENAME COLUMNS AND ADD COLUMNS(ingestion_date)
constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())
                                             #.withColumn("name", concat(col("fullname.fname"), lit(" "), col("fullname.lname")))

# --------------------------------------------------------------------------
# Step 4 - WRITE DATA TO DATALAKE AS PARQUET
constructor_final_df.write  \
                      .mode("overwrite") \
                      .parquet("/mnt/formula1dl/processed/constructors")
```

## 12.2 Multi-Line JSON
```json
[
  {
    "raceId": 841,
    "driverId": 153,
    "stop": 1,
    "lap": 1,
    "time": "17:05:23",
    "duration": 26.898,
    "milliseconds": 26898
  },
  {
    "raceId": 841,
    "driverId": 30,
    "stop": 1,
    "lap": 1,
    "time": "17:05:52",
    "duration": 25.091,
    "milliseconds": 25091
  }
]
```
```python
# ingestion/6.ingest_pit_stops_file

pit_stops_df = spark.read \
                    .schema(pit_stops_schema) \
                    .option("multiLine", True) \                # Handle Multiline JSON
                    .json("/mnt/formula1dl/raw/pit_stops.json")

```

# Section 13: Data Ingestion - Multiple Files
Coming soon..

# Section 15: Transformation - Filter & Join
Coming soon..

# Section 16: Aggregations
Coming soon..

