Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27517062

# Section 11: Data Ingestion - CSV

|      |    |                     |    |                |    |                     |    |         | 
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  |    |                     |    |                |    |                     |    | Parquet |
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | AVRO    |
| XML  |    | DataFrameReader API |    | DataFrame API  |    | DataFrameWriter API |    | Delta   |
| JDBC |    |                     |    | Data Types     |    |                     |    | JDBC    |
| ...  |    |                     |    | Row            |    |                     |    | ...     |
|      |    |                     |    | Column         |    |                     |    |         |
|      |    |                     |    | Functions      |    |                     |    |         |
|      |    |                     |    | Window         |    |                     |    |         |
|      |    |                     |    | Grouping       |    |                     |    |         |
|      |    |                     |    |                |    |                     |    |         |

| File Name    | File type                   |
|--------------|-----------------------------|
| Circuits     | CSV                         |
| Races        | CSV                         |
| Constructors | Single line JSON            |
| Results      | Single line JSON            |
| Drivers      | Single line Nested JSON     |
| Pitstops     | Multi line JSON             |
| Laptimes     | Split CSV Files             |
| Qualifying   | Split Multi line JSON Files |

## 11.1 Data Ingestion - CSV
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

## 11.2 DataFrameReader, Specify Schema, Select Columns, Rename Columns, Add Column, DataFrameWriter

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html

```python
# ingestion/1.ingest_circuits_file

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import col, current_timestamp

# --------------------------------------------------------------------------
# Step 1a - READ A CSV FILE USING SPARK DATAFRAMEREADER (Find Schema Automatically)
circuits_df = spark.read \
                  .option("header", True) \                       # Specify that there is a header present
                  .option("inferSchema", True) \                  # Set appropriate schema (Different job runs in background)
                  .schema(circuits_schema) \                      # Specify schema
                  .csv("dbfs://mnt/formula1dl/raw/circuits.csv")
# OR
# Step 1b - READ A CSV FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
circuits_schema = StructType(fields=[StructField("circuitId",   IntegerType(), False),
                                     StructField("circuitRef",  StringType(),  True),
                                     StructField("name",        StringType(),  True),
                                     StructField("location",    StringType(),  True),
                                     StructField("country",     StringType(),  True),
                                     StructField("lat",         DoubleType(),  True),
                                     StructField("lng",         DoubleType(),  True),
                                     StructField("alt",         IntegerType(), True),
                                     StructField("url",         StringType(),  True)
                                     #StructField("date",       DateType(),    True),
                                     #StructField("time",       StringType(),  True),
])
circuits_df = spark.read \
                  .option("header", True) \
                  .schema(circuits_schema) \
                  .csv("/mnt/formula1dl/raw/circuits.csv")

type(circuits_df)    # DataFrame

circuits_df.show()   # Displays all data in DF in Text format
  
display(circuits_df) # Displays all data in DF in Table format

circuits_df.printSchema()

# --------------------------------------------------------------------------
# Step 2 - SELECT ONLY REQUIRED COLUMNS
circuits_selected_df = circuits_df.select(\
                                    col("circuitId"), col("circuitRef"), col("name"), col("location"),
                                    col("country"), col("lat"), col("lng"), col("alt")
)
# OR
circuits_selected_df = circuits_df.select(\
                                    "circuitId", "circuitRef", "name" , "location",
                                    "country", "lat", "lng", "alt"
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df.circuitId, circuits_df.circuitRef, circuits_df.name, circuits_df.location,
                                    circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df["circuitId"], circuits_df["circuitRef"], circuits_df["name"], circuits_df["location"],
                                    circuits_df["country"], circuits_df["lat"], circuits_df["lng"], circuits_df["alt"]
)

# --------------------------------------------------------------------------
# Step 3 - RENAME EXISTING COLUMNS AS REQUIRED
circuits_renamed_df = circuits_selected_df
                                  .withColumnRenamed("circuitId", "circuit_id") \
                                  .withColumnRenamed("circuitRef", "circuit_ref") \
                                  .withColumnRenamed("lat", "latitude") \
                                  .withColumnRenamed("lng", "longitude") \
                                  .withColumnRenamed("alt", "altitude")

# --------------------------------------------------------------------------
# Step 4 - ADD NEW COLUMN (ingestion_date, gender) TO THE DATAFRAME
circuits_final_df = circuits_renamed_df
                                  .withColumn("ingestion_date", current_timestamp()) \
                                  #.withColumn("ingestion_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))
                                  .withColumns("gender", lit("Male"))

# --------------------------------------------------------------------------
# Step 5 - WRITE DATA TO DATALAKE AS PARQUET
# NOTE: If you do not have Databricks mounts, use the cluster-scoped authentication and abfss protocol as described in Section 6. Folder path will be:
# abfss://processed@formula1dl.dfs.core.windows.net/circuits
circuits_final_df.write \
                .mode("overwrite") \
                #.partitionBy("country")
                .parquet("/mnt/formula1dl/processed/circuits")

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))    # Read the output Parquet file

```

# Section 12: Data Ingestion - JSON
Coming soon..

# Section 13: Data Ingestion - Multiple Files
Coming soon..

# Section 15: Transformation - Filter & Join
Coming soon..

# Section 16: Aggregations
Coming soon..

