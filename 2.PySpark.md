Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27517062

# Section 11, 12, 13: Data Ingestion - CSV, JSON, Multiple Files

|      |    |                     |    |                |    |                     |    |         | 
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  |    |                     |    |                |    |                     |    | Parquet |
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | AVRO    |
| XML  |    | DataFrameReader API |    | DataFrame API  |    | DataFrameWriter API |    | Delta   |
| JDBC |    |                     |    | Data Types     |    |                     |    | JDBC    |
| ...  |    |                     |    | Row            |    |                     |    | ...     |
|      |    |                     |    | Column         |    |                     |    |         |
|      |    |                     |    | Functions      |    |                     |    |         |
|      |    |                     |    | Window         |    |                     |    |         |
|      |    |                     |    | Grouping       |    |                     |    |         |
|      |    |                     |    |                |    |                     |    |         |

| File Name    | File type                   |
|--------------|-----------------------------|
| Circuits     | CSV                         |
| Races        | CSV                         |
| Constructors | Single line JSON            |
| Results      | Single line JSON            |
| Drivers      | Single line Nested JSON     |
| Pitstops     | Multi line JSON             |
| Laptimes     | Split CSV Files             |
| Qualifying   | Split Multi line JSON Files |

# Section 11: Data Ingestion - CSV
## 11.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

## 11.2 DataFrameReader, Specify Schema, Select Columns, Rename Columns, Add Column, DataFrameWriter

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html

```python
# ingestion/1.ingest_circuits_file
# ingestion/2.ingest_races_file

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
from pyspark.sql.functions import col, current_timestamp

# --------------------------------------------------------------------------
# Step 1a - READ A CSV FILE USING SPARK DATAFRAMEREADER (Find Schema Automatically)
circuits_df = spark.read \
                  .option("header", True) \                       # Specify that there is a header present
                  .option("inferSchema", True) \                  # Set appropriate schema (Different job runs in background)
                  .schema(circuits_schema) \                      # Specify schema
                  .csv("dbfs://mnt/formula1dl/raw/circuits.csv")
# OR
# Step 1b - READ A CSV FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
circuits_schema = StructType(fields=[StructField("circuitId",   IntegerType(), False),
                                     StructField("circuitRef",  StringType(),  True),
                                     StructField("name",        StringType(),  True),
                                     StructField("location",    StringType(),  True),
                                     StructField("country",     StringType(),  True),
                                     StructField("lat",         DoubleType(),  True),
                                     StructField("lng",         DoubleType(),  True),
                                     StructField("alt",         IntegerType(), True),
                                     StructField("url",         StringType(),  True)
                                     #StructField("date",       DateType(),    True),
                                     #StructField("time",       StringType(),  True),
])
circuits_df = spark.read \
                  .option("header", True) \
                  .schema(circuits_schema) \
                  .csv("/mnt/formula1dl/raw/circuits.csv")

type(circuits_df)    # DataFrame

circuits_df.show()   # Displays all data in DF in Text format
  
display(circuits_df) # Displays all data in DF in Table format

circuits_df.printSchema()

# --------------------------------------------------------------------------
# Step 2 - SELECT ONLY REQUIRED COLUMNS
circuits_selected_df = circuits_df.select(\
                                    col("circuitId"), col("circuitRef"), col("name"), col("location"),
                                    col("country"), col("lat"), col("lng"), col("alt")
)
# OR
circuits_selected_df = circuits_df.select(\
                                    "circuitId", "circuitRef", "name" , "location",
                                    "country", "lat", "lng", "alt"
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df.circuitId, circuits_df.circuitRef, circuits_df.name, circuits_df.location,
                                    circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df["circuitId"], circuits_df["circuitRef"], circuits_df["name"], circuits_df["location"],
                                    circuits_df["country"], circuits_df["lat"], circuits_df["lng"], circuits_df["alt"]
)

# --------------------------------------------------------------------------
# Step 3 - RENAME EXISTING COLUMNS AS REQUIRED
circuits_renamed_df = circuits_selected_df
                                  .withColumnRenamed("circuitId", "circuit_id") \
                                  .withColumnRenamed("circuitRef", "circuit_ref") \
                                  .withColumnRenamed("lat", "latitude") \
                                  .withColumnRenamed("lng", "longitude") \
                                  .withColumnRenamed("alt", "altitude")

# --------------------------------------------------------------------------
# Step 4 - ADD NEW COLUMN (ingestion_date, gender) TO THE DATAFRAME
circuits_final_df = circuits_renamed_df
                                  .withColumn("ingestion_date", current_timestamp()) \
                                  #.withColumn("ingestion_timestamp", to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))
                                  .withColumns("gender", lit("Male"))

# --------------------------------------------------------------------------
# Step 5 - WRITE DATA TO DATALAKE AS PARQUET
# NOTE: If you do not have Databricks mounts, use the cluster-scoped authentication and abfss protocol as described in Section 6. Folder path will be:
# abfss://processed@formula1dl.dfs.core.windows.net/circuits
circuits_final_df.write \
                .mode("overwrite") \
                #.partitionBy("country")
                .parquet("/mnt/formula1dl/processed/circuits")

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))    # Read the output Parquet file

```

# Section 12: Data Ingestion - JSON
## 12.1 Overview
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html#pyspark.sql.DataFrameReader.json
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html
  
## 12.2 Single Line JSON
JSON File:\
```json
{"constructorId":1, "constructorRef":"mclaren", "name":"McLaren", "nationality":"British", "url":"www.mclaren.com", "fullname":{"fname":"Mc", "lname":"Laren"}}
{"constructorId":2, "constructorRef":"landrover", "name":"LandRover", "nationality":"Indian", "url":"www.landrover.com", "fullname":{"fname":"Land", "lname":"Rover"}}
```

```python
# ingestion/3.ingest_constructors_file
# ingestion/4.ingest_drivers_file

from pyspark.sql.functions import col, current_timestamp
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

# Step 1a - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
constructors_schema = "constructorId INT, constructorRef STRING, name STRING, nationality STRING, url STRING"
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# Step 1b - READ A JSON FILE USING SPARK DATAFRAMEREADER (Use User-Defined Schema)
#inner_fullname_schema = StructType(fields=[StructField("fname", StringType(), True),
#                                      StructField("lname", StringType(), True)
# ])
constructors_schema = StructType(fields=[StructField("constructorId",  IntegerType(), True), \
                                         StructField("constructorRef", StringType(), True), \
                                         StructField("name",           StringType(), True), \
                                         StructField("nationality",    StringType(), True), \
                                         StructField("url",            StringType(), True), \
                                         # StructField("time",         StringType(), True), \        # Syntax: "17:05:23"
                                         # StructField("fullname", inner_fullname_schema), \         # Nested JSON
])
constructor_df = spark.read \
                      .schema(constructors_schema) \
                      .json("/mnt/formula1dl/raw/constructors.json")

# --------------------------------------------------------------------------
# Step 2 - DROP UNWANTED COLUMNS FROM DATAFRAME
constructor_dropped_df = constructor_df.drop(col('url'))

# --------------------------------------------------------------------------
# Step 3 - RENAME COLUMNS AND ADD COLUMNS(ingestion_date)
constructor_final_df = constructor_dropped_df.withColumnRenamed("constructorId", "constructor_id") \
                                             .withColumnRenamed("constructorRef", "constructor_ref") \
                                             .withColumn("ingestion_date", current_timestamp())
                                             #.withColumn("name", concat(col("fullname.fname"), lit(" "), col("fullname.lname")))

# --------------------------------------------------------------------------
# Step 4 - WRITE DATA TO DATALAKE AS PARQUET
constructor_final_df.write  \
                      .mode("overwrite") \
                      .parquet("/mnt/formula1dl/processed/constructors")
```

## 12.2 Multi-Line JSON
```json
[
  {
    "raceId": 841,
    "driverId": 153,
    "stop": 1,
    "lap": 1,
    "time": "17:05:23",
    "duration": 26.898,
    "milliseconds": 26898
  },
  {
    "raceId": 841,
    "driverId": 30,
    "stop": 1,
    "lap": 1,
    "time": "17:05:52",
    "duration": 25.091,
    "milliseconds": 25091
  }
]
```
```python
# ingestion/6.ingest_pit_stops_file

pit_stops_df = spark.read \
                    .schema(pit_stops_schema) \
                    .option("multiLine", True) \                # Handle Multiline JSON
                    .json("/mnt/formula1dl/raw/pit_stops.json")
```

# Section 13: Data Ingestion - Multiple Files
## 13.1 Multiple Split CSV Files
- CSV file is split into multiple files, with no headers.
```
lap_times
+ lap_times_split_1.csv
+ lap_times_split_2.csv
+ lap_times_split_3.csv
```

lap_times_split_1.csv
```csv
841,20,1,1,"1:38:109",98109
841,20,2,1,"1:33:006",93006
841,20,3,1,"1:32:713",92713
```

Schema:
```
raceId        int(11)
driverId      int(11)
lap           int(11)
position      int(11)
time          varchar(255)
milliseconds  int(11)
```

```python
# ingestion/7.ingest_lap_times_file

lap_times_df = spark.read \
                    .schema(lap_times_schema) \
                    .csv("/mnt/formula1dl/raw/lap_times")                      # Handle Multiple CSV Files
# OR
lap_times_df = spark.read \
                    .schema(lap_times_schema) \
                    .csv("/mnt/formula1dl/raw/lap_times/lap_times_split*.csv")  # Handle Multiple CSV Files
```

## 13.2 Multiple Split Multiline JSON Files
- JSON file is split into multiple files, with no headers.
```
qualifying
+ qualifying_split_1.csv
+ qualifying_split_2.csv
+ qualifying_split_3.csv
```

qualifying_split_1.csv
```json
[
  {
    "qualifyId": 1,
    "raceId": 18,
    "driverId": 1,
    "constructorId": 1,
    "number": 22,
    "position": 1,
    "q1": "1:26:572",
    "q2": "1:25:187",
    "q3": "1:26:714"
  },
  {
    "qualifyId": 2,
    "raceId": 18,
    "driverId": 9,
    "constructorId": 2,
    "number": 4,
    "position": 2,
    "q1": "1:26:103",
    "q2": "1:25:315",
    "q3": "1:26:869"
  }
]
```

```python
# ingestion/8.ingest_qualifying_file

qualifying_df = spark.read \
                        .schema(qualifying_schema) \
                        .option("multiLine", True) \              # Handle Multiline JSON
                        .json("/mnt/formula1dl/raw/qualifying")   # Handle Multiple JSON Files
```

# Section 15: Transformation - Filter & Join

## 15.1 Filter
**Reference:**
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.filter.html#pyspark.sql.DataFrame.filter

```python
# /demo/1.filter_demo

%run "/includes/configuration"
races_df = spark.read.parquet(f"{processed_folder_path}/races")

races_filtered_df = races_df.filter("race_year = 2019 and round <=5").collect()
# OR
races_filtered_df = races_df.filter( (races_df.race_year = 2019) & (races_df.round <= 5)).collect()
# OR
races_filtered_df = races_df.filter( (races_df["race_year"] == 2019) & (races_df["round"] <= 5) ).collect()
```

## 15.2 Joins
**Reference:**
- https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join

```python
# /demo/2.join_demo
%run "/includes/configuration"

circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
                        .filter("circuit_id < 70") \
                        .withColumnRenamed("name", "circuit_name")
races_df = spark.read.parquet(f"{processed_folder_path}/races")
                        .filter("race_year = 2019") \
                        .withColumnRenamed("name", "race_name")

# Inner Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "inner") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Left Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "left") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Right Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "right") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Full Outer Join
race_circuits_df = circuits_df \
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "full") \
                        .select(circuits_df.circuit_name, circuits_df.location, circuits_df.country, races_df.race_name, races_df.round)
# Semi Joins
race_circuits_df = circuits_df
                        .join(races_df, circuits_df.circuit_id == races_df.circuit_id, "semi") 
# Anti Joins
race_circuits_df = races_df
                        .join(circuits_df, circuits_df.circuit_id == races_df.circuit_id, "anti") 
# Cross Joins
race_circuits_df = races_df
                        .crossJoin(circuits_df)
```

## 15.3 Setup Presentation Layer (similar to that in https://www.bbc.com/sport/formula1/2020/abu-dhabi-grand-prix/results)

| Driver	| Nationality | Number	| Team	| Grid	| Pits	| Fastest Lap	| Race Time	| PointsPts |
|---------|-------------|---------|-------|-------|-------|-------------|-----------|-----------|
| 11	| Netherlands | Max Verstappen	| 3333	| Red Bull	| 11	| 11	| 1:40.9581:40.958	| 1:36:28.645 | 1:36:28.645	| 2525 |
| 22	| Finland | Valtteri Bottas	| 7777	| Mercedes	| 22	| 11	| 1:41.1311:41.131	| 15.976 	| 1818 |
| 33	| Great Britain | Lewis Hamilton	| 4444	| Mercedes	| 33	| 11	| 1:41.4201:41.420 | 18.415 |	1515 |
| 44	| Thailand | Alexander Albon	| 2323	| Red Bull	| 55	| 11	| 1:41.2271:41.227	| 19.987 | 1212 |
| 55	| Great Britain | Lando Norris	| 44	| McLaren	| 44	| 11	| 1:41.9641:41.964	| 1:00.729 | 1010 |

In Microsoft Azure Storage Explorer create folder 'presentation', 
```
Pay-As-You-Go (az.adm1@outlook.com)
+ Storage Accounts
  + Storage Accounts
    + formula1dl (ADLS Gen2)
      + Blob Containers
        + processed
        + raw
        + presentation (******** Add this folder ********)
      + File Shares
      + Queues
      + Tables 
```

In Microsoft Azure Databricks, update file mount_adls_storage\
Workspace > formula1 > set-up > mount_adls_storage 
```
mount_adls("presentation")

dbutils.fs.ls("/mnt/formula1dl/presentation")
```

In Microsoft Azure Databricks, update file configuration \
Workspace > formula1 > includes > configuration 
```
presentation_folder_path = "/mnt/formula1dl/presentation"
```

### Join Race Results
In Microsoft Azure Databricks, create new notebook '1.race_results'\
Workspace > formula1 > trans > race_results

```python
# trans/1.race_results.py

%run "../includes/configuration"

from pyspark.sql.functions import current_timestamp

drivers_df = spark.read.parquet(f"{processed_folder_path}/drivers") \
                        .withColumnRenamed("number", "driver_number") \
                        .withColumnRenamed("name", "driver_name") \
                        .withColumnRenamed("nationality", "driver_nationality") 
constructors_df = spark.read.parquet(f"{processed_folder_path}/constructors") \
                        .withColumnRenamed("name", "team") 
circuits_df = spark.read.parquet(f"{processed_folder_path}/circuits") \
                        .withColumnRenamed("location", "circuit_location") 
races_df = spark.read.parquet(f"{processed_folder_path}/races") \
                        .withColumnRenamed("name", "race_name") \
                        .withColumnRenamed("race_timestamp", "race_date") 
results_df = spark.read.parquet(f"{processed_folder_path}/results") \
                        .withColumnRenamed("time", "race_time") 

# Join circuits to races
race_circuits_df = races_df \
                        .join(circuits_df, races_df.circuit_id == circuits_df.circuit_id, "inner") \
                        .select(races_df.race_id, races_df.race_year, races_df.race_name, races_df.race_date, circuits_df.circuit_location)

# Join results to all other dataframes
race_results_df = results_df \
                        .join(race_circuits_df, results_df.race_id == race_circuits_df.race_id) \
                        .join(drivers_df, results_df.driver_id == drivers_df.driver_id) \
                        .join(constructors_df, results_df.constructor_id == constructors_df.constructor_id)

final_df = race_results_df
                        .select("race_year", "race_name", "race_date", "circuit_location", "driver_name", "driver_number", \
                          "driver_nationality", "team", "grid", "fastest_lap", "race_time", "points", "position") \
                          .withColumn("created_date", current_timestamp())

display(final_df.filter("race_year == 2020 and race_name == 'Abu Dhabi Grand Prix'").orderBy(final_df.points.desc()))

final_df.write.mode("overwrite")
              .parquet(f"{presentation_folder_path}/race_results")
```

# Section 16: Aggregations
## 16.1 Simple Aggregate Functions, GroupBy, Window Functions
**References:**
- https://spark.apache.org/docs/3.1.1/api/python/reference/index.html
- https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#functions - Find by text "Aggregate" or "Rank" for Aggregate functions
- https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#window

```python
# /demo/2.join_demo
%run "/includes/configuration"

from pyspark.sql.functions import count, countDistinct, sum, desc, rank
from pyspark.sql.window import Window

# Built-in Aggregate functions
race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")
display(race_results_df)

demo_df = race_results_df.filter("race_year=2020")
display(demo_df)

demo_df.select(count("*")).show()

demo_df.select(count("race_name")).show()

demo_df.select(countDistinct("race_name")).show()

demo_df.select(sum("points")).show()

demo_df.filter("driver_name = 'Lewis Hamilton'") \
        .select(sum("points")).show()

demo_df.filter("driver_name = 'Lewis Hamilton'")
          .select(sum("points"), countDistinct("race_name")) \
          .withColumnRenamed("sum(points)", "total_points") \
          .withColumnRenamed("count(DISTINCT race_name)", "number_of_races") \
          .show()

# GroupBy
# It helps you Group the data based on the specified column.
# You cannot use 2 Aggregations on a DataFrame. So, use agg() function:
demo_df\
      .groupBy("driver_name") \
      .agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) \
      .show()

# Window Functions
demo_df = race_results_df.filter("race_year in (2019, 2020)")
demo_grouped_df = demo_df\
                    .groupBy("race_year", "driver_name") \
                    .agg(sum("points").alias("total_points"), countDistinct("race_name").alias("number_of_races")) 
display(demo_grouped_df)
driverRankSpec = Window.partitionBy("race_year").orderBy(desc("total_points"))
demo_grouped_df.withColumn("rank", rank().over(driverRankSpec)).show(100)

```

## 16.2 Driver Standings (Setup Presentation Layer similar to that in https://www.bbc.com/sport/formula1/drivers-world-championship/standings)

| Nationality 	| Driver        	 | Team	         | Wins	| Points |
|---------------|------------------|---------------|------|--------|
| Spain         | Fernando Alonso  |	Aston Martin |	2	  | 105    |
| United States | Logan Sargeant   |	Williams     |	3	  | 101    |
| Germany       | Nico Hulkenberg  |	Haas         |	0	  |  56    | 
| Mexico        | Sergio Perez     |	Red Bull     |	0	  | 47     |
| Australia     | Daniel Ricciardo |	RB           |	0	  | 40     |

```python
# trans/2.driver_standings.py

%run "../includes/configuration"

from pyspark.sql.functions import sum, when, count, col, desc, rank, asc
from pyspark.sql.window import Window

race_results_df = spark.read.parquet(f"{presentation_folder_path}/race_results")

driver_standings_df = race_results_df \
                        .groupBy("race_year", "driver_name", "driver_nationality", "team") \
                        .agg(
                            sum("points").alias("total_points"),
                            count(when(col("position") == 1, True)).alias("wins")
                        )
display(driver_standings_df.filter("race_year = 2020"))

driver_rank_spec = Window.partitionBy("race_year").orderBy(desc("total_points"), desc("wins"))
final_df = driver_standings_df.withColumn("rank", rank().over(driver_rank_spec))

display(final_df.filter("race_year = 2020"))

final_df.write.mode("overwrite").parquet(f"{presentation_folder_path}/driver_standings")
```

## 16.3 Constructor Standings
Coming soon..




# Section 21: Incremental Load
Coming soon..


