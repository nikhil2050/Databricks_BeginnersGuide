Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27517062

# Section 21: Data Ingestion - CSV

|      |    |                     |    |                |    |                     |    |         | 
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  |    |                     |    |                |    |                     |    | Parquet |
| JSON | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | AVRO    |
| XML  |    | DataFrameReader API |    | DataFrame API  |    | DataFrameWriter API |    | Delta   |
| JDBC |    |                     |    | Data Types     |    |                     |    | JDBC    |
| ...  |    |                     |    | Row            |    |                     |    | ...     |
|      |    |                     |    | Column         |    |                     |    |         |
|      |    |                     |    | Functions      |    |                     |    |         |
|      |    |                     |    | Window         |    |                     |    |         |
|      |    |                     |    | Grouping       |    |                     |    |         |
|      |    |                     |    |                |    |                     |    |         |

| File Name    | File type                   |
|--------------|-----------------------------|
| Circuits     | CSV                         |
| Races        | CSV                         |
| Constructors | Single line JSON            |
| Results      | Single line JSON            |
| Drivers      | Single line Nested JSON     |
| Pitstops     | Multi line JSON             |
| Laptimes     | Split CSV Files             |
| Qualifying   | Split Multi line JSON Files |

## 21.1 Data Ingestion - CSV
|      |    |                     |    |                |    |                     |    |         |
|------|----|---------------------|----|----------------|----|---------------------|----|---------|
| CSV  | -> | READ DATA           | -> | TRANSFORM DATA | -> | WRITE DATA          | -> | Parquet |

## 21.2 DataFrameReader

Reference:
- https://spark.apache.org/docs/latest/api/python/reference/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html
- https://spark.apache.org/docs/latest/api/python/index.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html
- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html

```python
# ingestion/1.ingest_circuits_file

# Step 1a - Read the CSV file using the spark dataframe reader (Find Schema)
circuits_df = spark.read \
                  .option("header", True) \                       # Specify that there is a header present
                  .option("inferSchema", True) \                  # Set appropriate schema (Different job runs in background)
                  .schema(circuits_schema) \                      # Specify schema
                  .csv("dbfs://mnt/formula1dl/raw/circuits.csv")
# OR
# Step 1b - Read the CSV file using the spark dataframe reader (Use User-Defined Schema)
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType
circuits_schema = StructType(fields=[StructField("circuitId", IntegerType(), False),
                                     StructField("circuitRef", StringType(), True),
                                     StructField("name", StringType(), True),
                                     StructField("location", StringType(), True),
                                     StructField("country", StringType(), True),
                                     StructField("lat", DoubleType(), True),
                                     StructField("lng", DoubleType(), True),
                                     StructField("alt", IntegerType(), True),
                                     StructField("url", StringType(), True)
])
circuits_df = spark.read \
                  .option("header", True) \
                  .schema(circuits_schema) \
                  .csv("/mnt/formula1dl/raw/circuits.csv")

type(circuits_df)    # DataFrame

circuits_df.show()   # Displays all data in DF in Text format
  
display(circuits_df) # Displays all data in DF in Table format

circuits_df.printSchema()

# Step 2 - Select only the required columns
from pyspark.sql.functions import col
circuits_selected_df = circuits_df.select(\
                                    col("circuitId"), col("circuitRef"), col("name"), col("location"),
                                    col("country"), col("lat"), col("lng"), col("alt")
)
# OR
circuits_selected_df = circuits_df.select(\
                                    "circuitId", "circuitRef", "name" , "location",
                                    "country", "lat", "lng", "alt"
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df.circuitId, circuits_df.circuitRef, circuits_df.name, circuits_df.location,
                                    circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt
)
# OR
circuits_selected_df = circuits_df.select(\
                                    circuits_df["circuitId"], circuits_df["circuitRef"], circuits_df["name"], circuits_df["location"],
                                    circuits_df["country"], circuits_df["lat"], circuits_df["lng"], circuits_df["alt"]
)

# Step 3 - Rename the columns as required
circuits_renamed_df = circuits_selected_df
                                  .withColumnRenamed("circuitId", "circuit_id") \
                                  .withColumnRenamed("circuitRef", "circuit_ref") \
                                  .withColumnRenamed("lat", "latitude") \
                                  .withColumnRenamed("lng", "longitude") \
                                  .withColumnRenamed("alt", "altitude")

# Step 4 - Add ingestion date to the dataframe
from pyspark.sql.functions import current_timestamp
circuits_final_df = circuits_renamed_df
                                  .withColumn("ingestion_date", current_timestamp()) 

# Step 5 - Write data to datalake as parquet
circuits_final_df.write.mode("overwrite").parquet("/mnt/formula1dl/processed/circuits")

display(spark.read.parquet("/mnt/formula1dl/processed/circuits"))


```






