Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27514596#overview

# Section 22: Data Lake

## 22.1 Disadvantages of Data Lake:
**References**:
- https://medium.com/@gauravthalpati/5-key-limitations-of-data-lakes-dc74f545a33a

1. No ACID Support
2. Data Discoverability Challenges
3. Failed Jobs leave partial files
4. Inconsistent reads
5. Unable to handle corrections to data
6. Unable to roll-back any data
7. Lack of ability remove data for GDPR etc
8. No history/versioning
9. Poor Query Performance
10. Poor Support for BI Workloads
11. Complex to set-up
12. Lambda architecture for streaming workloads

## 22.2 Data LakeHouse Architecture:

**Benefits of using LakeHouse Architecture:**
1. Provides ACID support
2. Provides History & Versioning (Helps stop unreliable DataSwamps being created in DataLakes)
3. Provides better performance
4. Simple Architecture (No Lambda architecture)
5. Handles all types of data
6. Cheap Cloud object storage
7. Uses open-source format
8. Supports all types of workloads (like BI, DataScience, ML)
9. Ability to use BI tools directly on them

**Data LakeHouse Architecture:**
- It is aimed at bringing best of both Data Warehouse and Data Lakes.
- They have been designed to provide better BI support as well as Data Science and Machine Learning support.
- Delta Lake is a Data Lake with ACID transaction controls.
- Due to the ability to have ACID transactions, we can now combine streaming and batch workloads too and eliminate the need for a Lambda architecture.
- This data could then be transformed more efficiently without the need to rewrite enter partitions in cases of reprocessing data or rewriting Data Lakes, in case of processing GDPR requests.
- Usually there are more than one stages of transformation, but I have only shown one here just for simplicity.
- Databricks Architecture has two stages, with the first stage transforming the raw data to provide a structure, perform any data cleansing and quality, etc., and return to a set of tables and they are called Silver tables.
- And the raw tables, I should say are called Bronze tables.
- So you go from Bronze to Silver, and then the data from the silver tables are aggregated to provide business meanings and they are called Gold tables.
- For simplicity, as I said, I have used only one box here.
- Data from any of these set of Delta tables could then be used for Data Science and Machine Learning workloads.
- Delta Lake also provides connectors to BI tools such as Power BI and Tableau.
- Also, we can have roles and access controls defined for data governance.
- This removes the need for copying the data to a Data Warehouse.
- So this is basically the Lakehouse Architecture, but there are still projects running BI reports from a relational database rather than the Delta Lake due to the lack of performance.
- Delta Lake has been in existence since 2018 and rapidly evolving, but the performance still can't be compared to a Data Warehouse when it comes to reporting.
- So if you're looking for that level of performance, which you get from a Data Warehouse, you will still have to copy the data to a Data Warehouse.

|               |    |        |    |            |    |            |    |            | 
|---------------|----|--------|----|------------|----|------------|--- |------------|
| Operational + |    |        |    |            |    |            |    |            |
| External data |    |        |    |            |    |            |    |            |
| CSV           |    |        |    |            |    |            |    |            |
| JSON          | -> | INGEST | -> | DELTA LAKE | -> | TRANSFORM  | -> | DELTA LAKE |
| XML           |    |        |    |            |    |            |    |            |
| JDBC          |    |        |    | Parquet    |    |            |    | Parquet    |
| Files         |    |        |    | AVRO       |    |            |    | AVRO       |
| Video         |    |        |    | JDBC       |    |            |    | JDBC       |

![alt text](resources/DeltaLakeArchitecture.png?raw=false)

## 22.3 Read & Write to Delta Lake

In Microsoft Azure Storage Explorer create folder 'demo', 
```
Pay-As-You-Go (az.adm1@outlook.com)
+ Storage Accounts
  + formula1dl (ADLS Gen2)
    + Blob Containers
      + demo (******** Add this folder ********)
      + processed
      + raw
      + presentation
    + File Shares
    + Queues
    + Tables 
```

In Microsoft Azure Databricks, update file mount_adls_storage\
Workspace > formula1 > set-up > mount_adls_storage 
```
mount_adls("demo")

dbutils.fs.ls("/mnt/formula1dl/demo")
```

**References:**
- https://docs.delta.io/latest/index.html
- https://docs.delta.io/latest/delta-batch.html#write-to-a-table
- https://learn.microsoft.com/en-us/azure/databricks/delta/delta-change-data-feed

```python
# /demo/10.delta_lake_demo.py

%sql
CREATE DATABASE IF NOT EXISTS f1_demo \
      LOCATION '/mnt/formula1dl/demo';

results_df = spark.read \
                  .option("inferSchema", True) \
                  .json("/mnt/formula1dl/raw/2021-03-28/results.json")

# ---------------------------------------------------------------------------------------
# Step 1. Write data to delta lake (Managed table)
results_df.write.format("delta") \
                  .mode("overwrite") \
                  .saveAsTable("f1_demo.results_managed")        # database.tablename

# Folder is created in MS Azure Storage Explorer : demo/results_managed

# -----------------------------------------------------
# 2. Read data from delta lake (Managed/External Table)
%sql SELECT * FROM f1_demo.results_managed;

# ---------------------------------------------------------------------------------------
# 3. Write data to delta lake (External table)

results_df.write.format("delta")
                  .mode("overwrite")
                  .save("/mnt/formula1dl/demo/results_external")
# OR
%sql
CREATE TABLE f1_demo.results_external
      USING DELTA
      LOCATION '/mnt/formula1dl/demo/results_external'

# Folder is created in MS Azure Storage Explorer : demo/results_external

# -----------------------------------------------------
# 4. Read data from Delta lake (External Table)
results_external_df = spark.read.format("delta") \
                  .load("/mnt/formula1dl/demo/results_external")
# OR
%sql SELECT * FROM f1_demo.results_external

# ---------------------------------------------------------------------------------------
# 5. Write data to Partition folders
results_df.write.format("delta") \
                .mode("overwrite") \
                .partitionBy("constructorId") \              *** here
                .saveAsTable("f1_demo.results_partitioned")

# # Folder is created in MS Azure Storage Explorer : demo/results_partitioned
%sql
SHOW PARTITIONS f1_demo.results_partitioned

```

## 22.4 Updates, Deletes on Delta Lake

**References:**
- UPDATE: https://docs.delta.io/latest/delta-update.html#update-a-table

```python
# /demo/10.delta_lake_demo.py

from delta.tables import DeltaTable

# ---------------------------------------------------------------------------------------
# 1. UPDATE DELTA TABLE
%sql
UPDATE f1_demo.results_managed
      SET points = 11 - position
      WHERE position <= 10
# OR 
deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")
deltaTable.update("position <= 10", { "points": "21 - position" } ) 

# ---------------------------------------------------------------------------------------
# 2. DELETE FROM DELTA TABLE
%sql
DELETE FROM f1_demo.results_managed
      WHERE position > 10;
# OR
deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/results_managed")
deltaTable.delete("points = 0") 
```

## 22.4 Upserts(using Merge) on Delta Lake

It lets you:
1. Insert any data
2. Update any existing records from which new data has been received
3. Apply any delete 

**References:**
- UPDATE: https://docs.delta.io/latest/delta-update.html#update-a-table
- UPSERT: https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge

```python
# /demo/10.delta_lake_demo.py

from pyspark.sql.functions import current_timestamp
from delta.tables import DeltaTable

# Preparing a MERGE table

%sql
CREATE TABLE IF NOT EXISTS f1_demo.drivers_merge (
    driverId INT, forename STRING, surname STRING, dob DATE,
    createdDate DATE,  updatedDate DATE)                            # ***New columns added
    USING DELTA
| empId | forename | surname | age | createdDate | updatedDate |

# ---------------------------------------------------------------------------------------
# Usecase #1: Inserting data with Merge
# Merging drivers_day1 with drivers_merge

drivers_day1_df = spark.read.option("inferSchema", True) \
                        .json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
                        .filter("driverId <= 10") \
                        .select("driverId", "dob", "name.forename", "name.surname")
drivers_day1_df.createOrReplaceTempView("drivers_day1")
| empId | forename | surname | age |
|-------|----------|---------|-----|
| 1     | Jetha    | Gada    | 51  |
| 2     | Aatmaram | Bhide   | 52  |
| 3     | Roshan   | Sodi    | 53  |
| 4     | Hans     | Hathi   | 54  |
| 5     | Sundar   | Lal     | 55  |
| 6     | Popat    | Pande   | 56  |
| 7     | Taarak   | Mehta   | 57  |
| 8     | Krishnan | Iyer    | 58  |
| 9     | Chalu    | Pande   | 59  |
| 10    | Natu     | Kaka    | 60  |

%sql
MERGE INTO f1_demo.drivers_merge tgt
  USING drivers_day1 upd
  ON (tgt.driverId = upd.driverId)
  WHEN MATCHED THEN
    UPDATE SET
        tgt.dob = upd.dob, tgt.forename = upd.forename, tgt.surname = upd.surname, tgt.updatedDate = current_timestamp
  WHEN NOT MATCHED
    THEN INSERT (driverId, dob, forename,surname,createdDate )
        VALUES (driverId, dob, forename,surname, current_timestamp)
| empId | forename | surname | age | createdDate | updatedDate |
|-------|----------|---------|-----|-------------|-------------|
| 1     | Jetha    | Gada    | 51  | 2024-02-18  | null        |    **Record Added
| 2     | Aatmaram | Bhide   | 52  | 2024-02-18  | null        |    **Record Added
| 3     | Roshan   | Sodi    | 53  | 2024-02-18  | null        |    **Record Added
| 4     | Hans     | Hathi   | 54  | 2024-02-18  | null        |    **Record Added
| 5     | Sundar   | Lal     | 55  | 2024-02-18  | null        |    **Record Added
| 6     | Popat    | Pande   | 56  | 2024-02-18  | null        |    **Record Added
| 7     | Taarak   | Mehta   | 57  | 2024-02-18  | null        |    **Record Added
| 8     | Krishnan | Iyer    | 58  | 2024-02-18  | null        |    **Record Added
| 9     | Chalu    | Pande   | 59  | 2024-02-18  | null        |    **Record Added
| 10    | Natu     | Kaka    | 60  | 2024-02-18  | null        |    **Record Added

# ---------------------------------------------------------------------------------------
# Usecase #2a: SparkSQL - Update any existing records from which new data has been received
# Merging drivers_day2 with drivers_merge

drivers_day2_df = spark.read.option("inferSchema", True) \
                      .json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
                      .filter("driverId BETWEEN 6 AND 15") \
                      .select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))
drivers_day2_df.createOrReplaceTempView("drivers_day2")
| empId | forename | surname | age |
| 6     | POPAT    | PANDE   | 56  |
| 7     | TAARAK   | MEHTA   | 57  |
| 8     | KRISHNAN | IYER    | 58  |
| 9     | CHALU    | PANDE   | 59  |
| 10    | NATU     | KAKA    | 60  |
| 11    | DAYA     | GADA    | 41  |
| 12    | MADHVI   | BHIDE   | 42  |
| 13    | ROSHNI   | SODI    | 43  |
| 14    | KOMAL    | HATHI   | 44  |
| 15    | CARRIE   | IRWIN   | 45  |

%sql
MERGE INTO f1_demo.drivers_merge tgt
	USING drivers_day2 upd                                            # Change drivers_day1 -> drivers_day2
	ON (tgt.driverId = upd.driverId)
	WHEN MATCHED THEN
	  UPDATE SET tgt.dob = upd.dob,
				 tgt.forename = upd.forename,
				 tgt.surname = upd.surname,
				 tgt.updatedDate = current_timestamp
	WHEN NOT MATCHED
	  THEN INSERT (driverId, dob, forename,surname,createdDate ) VALUES (driverId, dob, forename,surname, current_timestamp)
| empId | forename | surname | age | createdDate | updatedDate |
|-------|----------|---------|-----|-------------|-------------|
| 1     | Jetha    | Gada    | 51  | 2024-02-18  | null        |    **Record Unchanged
| 2     | Aatmaram | Bhide   | 52  | 2024-02-18  | null        |    **Record Unchanged
| 3     | Roshan   | Sodi    | 53  | 2024-02-18  | null        |    **Record Unchanged
| 4     | Hans     | Hathi   | 54  | 2024-02-18  | null        |    **Record Unchanged
| 5     | Sundar   | Lal     | 55  | 2024-02-18  | null        |    **Record Unchanged
| 6     | POPAT    | PANDE   | 56  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 7     | TAARAK   | MEHTA   | 57  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 8     | KRISHNAN | IYER    | 58  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 9     | CHALU    | PANDE   | 59  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 10    | NATU     | KAKA    | 60  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 11    | DAYA     | GADA    | 41  | 2024-02-18  | null        |    **Record Added
| 12    | MADHVI   | BHIDE   | 42  | 2024-02-18  | null        |    **Record Added
| 13    | ROSHNI   | SODI    | 43  | 2024-02-18  | null        |    **Record Added
| 14    | KOMAL    | HATHI   | 44  | 2024-02-18  | null        |    **Record Added
| 15    | CARRIE   | IRWIN   | 45  | 2024-02-18  | null        |    **Record Added

# --------------------------------------
# Usecase #2b: PySpark - Update any existing records from which new data has been received
# Merging drivers_day3 with drivers_merge

drivers_day3_df = spark.read.option("inferSchema", True) \
                    .json("/mnt/formula1dl/raw/2021-03-28/drivers.json") \
                    .filter("driverId BETWEEN 1 AND 5 OR driverId BETWEEN 16 AND 20") \
                    .select("driverId", "dob", upper("name.forename").alias("forename"), upper("name.surname").alias("surname"))
| empId | forename | surname | age |
| 1     | JETHA    | GADA    | 51  |
| 2     | AATMARAM | BHIDE   | 52  |
| 3     | ROSHAN   | SODI    | 53  |
| 4     | HANS     | HATHI   | 54  |
| 5     | SUNDAR   | LAL     | 55  |
| 16    | TAPU     | GADA    | 16  |
| 17    | SONU     | BHIDE   | 17  |
| 18    | GOGI     | SODI    | 18  |
| 19    | PINKU    | SAHAY   | 19  |
| 20    | GOLI     | HATHI   | 20  |

deltaTable = DeltaTable.forPath(spark, "/mnt/formula1dl/demo/drivers_merge")

deltaTable.alias("tgt")
          .merge(drivers_day3_df.alias("upd"), "tgt.driverId = upd.driverId") \
          .whenMatchedUpdate(set = {"dob": "upd.dob", "forename": "upd.forename", "surname": "upd.surname", "updatedDate": "current_timestamp()" } ) \
          .whenNotMatchedInsert(values =
                                {
                                  "driverId": "upd.driverId",
                                  "dob": "upd.dob",
                                  "forename" : "upd.forename", 
                                  "surname" : "upd.surname", 
                                  "createdDate": "current_timestamp()"
                                }
                              ) \
          .execute()
| empId | forename | surname | age | createdDate | updatedDate |
| 1     | JETHA    | GADA    | 51  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 2     | AATMARAM | BHIDE   | 52  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 3     | ROSHAN   | SODI    | 53  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 4     | HANS     | HATHI   | 54  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 5     | SUNDAR   | LAL     | 55  | 2024-02-18  | 2024-02-18  |    **Record Updated
| 16    | TAPU     | GADA    | 16  | 2024-02-18  |             |    **Record Added
| 17    | SONU     | BHIDE   | 17  | 2024-02-18  |             |    **Record Added
| 18    | GOGI     | SODI    | 18  | 2024-02-18  |             |    **Record Added
| 19    | PINKU    | SAHAY   | 19  | 2024-02-18  |             |    **Record Added
| 20    | GOLI     | HATHI   | 20  | 2024-02-18  |             |    **Record Added

SELECT * FROM f1_demo.drivers_merge;
```

## 22.5 History, Time Travel, Vacuum, Restoring deleted data

```python
# /demo/10.delta_lake_demo.py

# Usecase #1: HISTORY & VERSIONING
%sql
DESC HISTORY f1_demo.drivers_merge;
| version | timestamp | userId | userName | operation | operationParameters | job | notebook | clusterId | readVersion | isolationLevel | isBlindAppend | operationMetrics |
| 3 | 2024-02-18T17:25:01.000+0000 | 987654321987 | az.adm1@outlook.com | MERGE | {...} | null | {"notebookbookId":"1234567890" | 4321-7654321-tabu272 | 2 | WriteSerializable | false | {...} |

%sql
SELECT * FROM f1_demo.drivers_merge VERSION AS OF 2;

# ---------------------------------------------------------------------------------------
# Usecase #2: TIME TRAVEL
%sql
SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';
# OR
%python
df = spark.read.format("delta").option("timestampAsOf", '2021-06-23T15:40:33.000+0000').load("/mnt/formula1dl/demo/drivers_merge")

# ---------------------------------------------------------------------------------------
# Usecase #3: VACUUM
%sql
VACUUM f1_demo.drivers_merge;								# By default Vacuum retains data updo 7 days
SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';	# Data is seen

# Set Vacuum of 0 hours
SET spark.databricks.delta.retentionDurationCheck.enabled = false;			# For hours less that 168 hours, add this check
VACUUM f1_demo.drivers_merge RETAIN 0 HOURS;
SELECT * FROM f1_demo.drivers_merge TIMESTAMP AS OF '2021-06-23T15:40:33.000+0000';	# FileReadException occurs
DESC HISTORY f1_demo.drivers_merge;							# All prior History is displayed. It is never erased

# ---------------------------------------------------------------------------------------
# Usecase #4: RESTORING DELETED DATA
%sql
DELETE FROM f1_demo.drivers_merge WHERE driverId = 1;

MERGE INTO f1_demo.drivers_merge tgt
	USING f1_demo.drivers_merge VERSION AS OF 3 src
	   ON (tgt.driverId = src.driverId)
	WHEN NOT MATCHED THEN
	   INSERT *

```

## 22.6 Delta Lake Transaction Log

```python
# /demo/10.delta_lake_demo.py

%sql
CREATE TABLE IF NOT EXISTS f1_demo.drivers_txn (
    driverId INT, dob DATE, forename STRING, surname STRING, createdDate DATE, updatedDate DATE )
    USING DELTA

# Note: All this history data is stored in Azure /demo/drivers_txn/_delta_log/000000000000000000000.json (& ..../000000000000000000000.crc is also created)

DESC HISTORY f1_demo.drivers_merge;	

# File structure:
# demo
# + drivers_txn
# + + _delta_log
# + + + __tmp_path_dir
# + + + 000000000000000000000.crc
# + + + 000000000000000000000.json

# ---------------------------------------------------------------------------------------
INSERT INTO f1_demo.drivers_txn
    SELECT * FROM f1_demo.drivers_merge WHERE driverId = 1;

# Note: All this history data is stored in MS Azure Storage Explorer: /demo/drivers_txn/_delta_log/000000000000000000001.json (& ..../000000000000000000001.crc is also created)
#        Also, there is a parquet file created at location: /demo/drivers_txn/part-00000-bad3ebda-4321-8765-1098-9876-10987654321.snappy.parquet.
#        There is only 1 file because there is only 1 partition.

DESC HISTORY f1_demo.drivers_txn;

# ------------------------------------------
INSERT INTO f1_demo.drivers_txn
    SELECT * FROM f1_demo.drivers_merge
    WHERE driverId = 2;

# Note: All this history data is stored in MS Azure Storage Explorer: /demo/drivers_txn/_delta_log/000000000000000000002.json (& ..../000000000000000000002.crc is also created)
#        Also, there is a parquet file created at location: /demo/drivers_txn/part-00000-bad3ebda-4321-8765-1098-9876-10987654322.snappy.parquet
#        There is only 1 file because there is only 1 partition.

# ------------------------------------------
for driver_id in range(3, 20):
  spark.sql(f"""INSERT INTO f1_demo.drivers_txn
                SELECT * FROM f1_demo.drivers_merge
                WHERE driverId = {driver_id}""");

# Note: All this history data is stored in MS Azure Storage Explorer: /demo/drivers_txn/_delta_log/000000000000000000003.json (& ..../000000000000000000003.crc is also created) and more (here until XX0016.)
#        Moreover there is a CHECKPOINT file created at 10th, which is combination until 10th file: /demo/drivers_txn/_delta_log/000000000000000000010.checkpoint.parquet
#        Overall, it reads less than 9 transaction files
#        Also, there is a parquet file created at location: /demo/drivers_txn/part-00000-bad3ebda-4321-8765-1098-9876-10987654322.snappy.parquet
#        There is only 1 file because there is only 1 partition.

```

## 22.7 Convert Parquet file to Delta

```python
# /demo/10.delta_lake_demo.py

%sql
CREATE TABLE IF NOT EXISTS f1_demo.drivers_convert_to_delta (
        driverId INT, dob DATE, forename STRING, surname STRING, createdDate DATE, updatedDate DATE)
    USING PARQUET;

# Folder for all history data is created in MS Azure Storage Explorer: /demo/drivers_convert_to_delta

%sql
INSERT INTO f1_demo.drivers_convert_to_delta
        SELECT * FROM f1_demo.drivers_merge;

%sql
CONVERT TO DELTA f1_demo.drivers_convert_to_delta;

df = spark.table("f1_demo.drivers_convert_to_delta");
df.write.format("parquet").save("/mnt/formula1dl/demo/drivers_convert_to_delta_new");

%sql
CONVERT TO DELTA parquet.`/mnt/formula1dl/demo/drivers_convert_to_delta_new`;
```





