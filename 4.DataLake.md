Reference: https://adp-gptlearning.udemy.com/course/azure-databricks-spark-core-for-data-engineers/learn/lecture/27514596#overview

# Section 22: Data Lake

## 22.1 Disadvantages of Data Lake:
**References**:
- https://medium.com/@gauravthalpati/5-key-limitations-of-data-lakes-dc74f545a33a

1. No ACID Support
2. Data Discoverability Challenges
3. Failed Jobs leave partial files
4. Inconsistent reads
5. Unable to handle corrections to data
6. Unable to roll-back any data
7. Lack of ability remove data for GDPR etc
8. No history/versioning
9. Poor Query Performance
10. Poor Support for BI Workloads
11. Complex to set-up
12. Lambda architecture for streaming workloads

## 22.2 Data LakeHouse Architecture:

**Benefits of using LakeHouse Architecture:**
1. Provides ACID support
2. Provides History & Versioning (Helps stop unreliable DataSwamps being created in DataLakes)
3. Provides better performance
4. Simple Architecture (No Lambda architecture)
5. Handles all types of data
6. Cheap Cloud object storage
7. Uses open-source format
8. Supports all types of workloads (like BI, DataScience, ML)
9. Ability to use BI tools directly on them

**Data LakeHouse Architecture:**
- It is aimed at bringing best of both Data Warehouse and Data Lakes.
- They have been designed to provide better BI support as well as Data Science and Machine Learning support.
- Delta Lake is a Data Lake with ACID transaction controls.
- Due to the ability to have ACID transactions, we can now combine streaming and batch workloads too and eliminate the need for a Lambda architecture.
- This data could then be transformed more efficiently without the need to rewrite enter partitions in cases of reprocessing data or rewriting Data Lakes, in case of processing GDPR requests.
- Usually there are more than one stages of transformation, but I have only shown one here just for simplicity.
- Databricks Architecture has two stages, with the first stage transforming the raw data to provide a structure, perform any data cleansing and quality, etc., and return to a set of tables and they are called Silver tables.
- And the raw tables, I should say are called Bronze tables.
- So you go from Bronze to Silver, and then the data from the silver tables are aggregated to provide business meanings and they are called Gold tables.
- For simplicity, as I said, I have used only one box here.
- Data from any of these set of Delta tables could then be used for Data Science and Machine Learning workloads.
- Delta Lake also provides connectors to BI tools such as Power BI and Tableau.
- Also, we can have roles and access controls defined for data governance.
- This removes the need for copying the data to a Data Warehouse.
- So this is basically the Lakehouse Architecture, but there are still projects running BI reports from a relational database rather than the Delta Lake due to the lack of performance.
- Delta Lake has been in existence since 2018 and rapidly evolving, but the performance still can't be compared to a Data Warehouse when it comes to reporting.
- So if you're looking for that level of performance, which you get from a Data Warehouse, you will still have to copy the data to a Data Warehouse.

|               |    |        |    |            |    |            |    |            | 
|---------------|----|--------|----|------------|----|------------|--- |------------|
| Operational + |    |        |    |            |    |            |    |            |
| External data |    |        |    |            |    |            |    |            |
| CSV           |    |        |    |            |    |            |    |            |
| JSON          | -> | INGEST | -> | DELTA LAKE | -> | TRANSFORM  | -> | DELTA LAKE |
| XML           |    |        |    |            |    |            |    |            |
| JDBC          |    |        |    | Parquet    |    |            |    | Parquet    |
| Files         |    |        |    | AVRO       |    |            |    | AVRO       |
| Video         |    |        |    | JDBC       |    |            |    | JDBC       |

![alt text](resources/DeltaLakeArchitecture.png?raw=false)

## 22.3 Read & Write to Delta Lake

In Microsoft Azure Storage Explorer create folder 'demo', 
```
Pay-As-You-Go (az.adm1@outlook.com)
+ Storage Accounts
  + formula1dl (ADLS Gen2)
    + Blob Containers
      + demo (******** Add this folder ********)
      + processed
      + raw
      + presentation
    + File Shares
    + Queues
    + Tables 
```

In Microsoft Azure Databricks, update file mount_adls_storage\
Workspace > formula1 > set-up > mount_adls_storage 
```
mount_adls("demo")

dbutils.fs.ls("/mnt/formula1dl/demo")
```

**References:**
- https://docs.delta.io/latest/index.html
- https://docs.delta.io/latest/delta-batch.html#write-to-a-table
- https://learn.microsoft.com/en-us/azure/databricks/delta/delta-change-data-feed

```python
# /demo/10.delta_lake_demo.py

%sql
CREATE DATABASE IF NOT EXISTS f1_demo \
      LOCATION '/mnt/formula1dl/demo';

results_df = spark.read \
                  .option("inferSchema", True) \
                  .json("/mnt/formula1dl/raw/2021-03-28/results.json")

# ---------------------------------------------------------------------------------------
# Step 1. Write data to delta lake (Managed table)
results_df.write.format("delta") \
                  .mode("overwrite") \
                  .saveAsTable("f1_demo.results_managed")        # database.tablename

# Folder is created in MS Azure Storage Explorer : demo/results_managed

# -----------------------------------------------------
# 2. Read data from delta lake (Managed/External Table)
%sql SELECT * FROM f1_demo.results_managed;

# ---------------------------------------------------------------------------------------
# 3. Write data to delta lake (External table)

results_df.write.format("delta")
                  .mode("overwrite")
                  .save("/mnt/formula1dl/demo/results_external")
# OR
%sql
CREATE TABLE f1_demo.results_external
      USING DELTA
      LOCATION '/mnt/formula1dl/demo/results_external'

# Folder is created in MS Azure Storage Explorer : demo/results_external

# -----------------------------------------------------
# 4. Read data from Delta lake (External Table)
results_external_df = spark.read.format("delta") \
                  .load("/mnt/formula1dl/demo/results_external")
# OR
%sql SELECT * FROM f1_demo.results_external

# ---------------------------------------------------------------------------------------
# 5. Write data to Partition folders
results_df.write.format("delta") \
                .mode("overwrite") \
                .partitionBy("constructorId") \              *** here
                .saveAsTable("f1_demo.results_partitioned")

# # Folder is created in MS Azure Storage Explorer : demo/results_partitioned
%sql
SHOW PARTITIONS f1_demo.results_partitioned



```




















